{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><strong><font size=+4>HERA Memo 69: H1C IDR 2.2</font>\n",
    "<br><br><em><font size=+1>Calibrated, Flagged, and LST-Binned HERA Internal Data Release</font></strong></em></center>\n",
    "<br><br>\n",
    "<center><strong><font size=+2>Josh Dillon</font><br><em>On behalf of the HERA Analysis Team</em></strong></center>\n",
    "<br><center><strong><font size=+1>July 31, 2019</font></strong></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H1C IDR 2.2 is a re-release of 18 days of calibrated data products from late 2017, part of [HERA](http://reionization.org/)'s first science observing season. It follows up on IDR 2.1, our first internal data release. That release was [described in this memo at reionization.org](http://reionization.org/wp-content/uploads/2018/07/IDR2.1_Memo_v2.html). This memo builds on that memo, and though the overlap is substantial, ID2.2 has a number of improvements:\n",
    "\n",
    "* **File format change**: We've moved form using miriad files to uvh5 files. This enables improved partial I/O.\n",
    "* **Python 3 support**: All modules (at least for now) support both Python 2 and 3 and the pipeline was run on Python 3.7.\n",
    "* **4-pol data files, 2-pol calibration files**: All data files now contain all four instrumental polarizations, all calibration files now contain both antenna polarizations.\n",
    "* **Bad antennas included in calibration files**: Flagged antennas are now included in calibration files, through their flags are all set to `True`. Previously, they were simply excluded.\n",
    "* **Fewer visibility files saved**: With the exception of delay-filtered data, we no longer produce any of the day-by-day calibrated/flagged data products that can be trivially reconstructed from calibration files and raw data. This saves considerable disk space. It is not true of LST-binned data.\n",
    "* **Improved redundant calibration**: Redundant calibration is now performed with an generalized firstcal, a new pure-python `omnical`, and proper normalization of $\\chi^2 / DoF$. The visibility solutions from `omnical` are updated to include absolute calibration in flagging later in the pipeline.\n",
    "* **Improved absolute calibration**: The externally calibrated set of \"model\" visibilities has been improved by including more sources and ignoring short baselines (under 40 m). Also, abscal calibration files now contain their own $\\chi^2$ and $\\chi^2$ per antenna, which compare calibrated data to the abscal model.\n",
    "* **Improved RFI flagging**: RFI flagging has been systematized and rationalized to produce a single flag waterfall (per time and frequency, but not per antenna or baseline) using metrics of \"outlierness\" from both raw data and calibration data products, combined in quadrature. These metrics are also used to pick entire channels or integrations to remove, based on the statistics of a whole day.\n",
    "* **Improved calibration smoothing**: Instead of smoothing in time and then in frequency, we now perform a 2D smoothing using an iterative CLEAN-like algorithm.\n",
    "* **Various new ancillary day-by-data data products**:\n",
    "    * Multi-frequency synthesis imaging using CASA\n",
    "    * Fits to cable reflections\n",
    "    * Estimates of the noise on calibrated data\n",
    "    * Extracted raw autocorrelations\n",
    "    * Absolutely-calibrated omnical visibility solutions\n",
    "* **UPDATE 12/3/19:** This notebook has been updated to run on the most recent version of `hera_cal`, which now supports polarizations denoted by cardinal directions (`'ee'`, `'nn'`, etc.) in favor of `'xx'` and `'yy'` polarizations when `x_orientation` is defined in the data files.\n",
    "\n",
    "This `jupyter notebook` memo is designed to be run when logged into NRAO. It is available as a [.ipynb via github](https://github.com/HERA-Team/hera_sandbox/blob/master/jsd/IDR2_2/IDR2.2_Memo.ipynb). In addition to the standard python libraries, it requires [pyuvdata](https://github.com/HERA-Team/pyuvdata), [hera_cal](https://github.com/HERA-Team/hera_cal), and [hera_qm](https://github.com/HERA-Team/hera_qm/). For more information on access, see the [HERA wiki](http://hera.pbworks.com/w/page/119477181/NRAO%20Computing). For ongoing discussions, join the [#hera-analysis Slack channel](https://eoranalysis.slack.com/messages/C3ZPGMG3E) or our \n",
    "telecons on Wednesdays at 10am Pacific in the HERA Zoom room."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import print_function, division, absolute_import\n",
    "import os\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The day-by-day data, along with the pipeline settings file used (`makeflow/idr2_2.toml`), and the LST-binned data can be found in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JD = 2458098.43869"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_folder = '/lustre/aoc/projects/hera/H1C_IDR2/IDR2_2/{}'.format(int(JD))\n",
    "raw_data_fn = 'zen.{}.HH.uvh5'.format(JD)\n",
    "\n",
    "hera_pkgs = '/users/heramgr/hera_software/'\n",
    "if not os.path.exists(analysis_folder): # working locally\n",
    "    local_comp = True\n",
    "    analysis_folder = '/Users/matyasmolnar/Downloads/HERA_Data/hc_analysis/sample_calib'\n",
    "    hera_pkgs = '/Users/matyasmolnar/Downloads/HERA_Data/hera_packages/'\n",
    "else:\n",
    "    local_comp = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding and Loading Data\n",
    "H1C IDR 2.2 includes 18 nearly-consecutive nights of data. All data products are sorted by JD into the following folders on the NRAO servers, which also contain softlinks to raw H1C IDR 2 data in the `.uvh5` format and antenna metrics released by the commissioning team. Each night has 73 raw visibility files. Each file has 4 polarizations, 1024 frequency channels and (usually) 60 integrations, each 10.7374 seconds.\n",
    "\n",
    "More information about the data products blessed by the commissioning team team are [on the HERA wiki](http://hera.pbworks.com/w/page/123874272/H1C_IDR2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HERA collaboration uses `pyuvdata` (for more, see [pyuvdata's documentation](http://pyuvdata.readthedocs.io/en/latest/)) as a standard pythonic interface between data and calibration file formats and in-memory objects. The HERA Analysis team uses subclasses of pyuvdata objects with a number of extra capabilities, namely `HERAData` and `HERACal` objects.\n",
    "\n",
    "For a more complete demonstration of the capabilities of the `hera_cal.io` module, refer to this [example notebook](https://github.com/HERA-Team/hera_cal/blob/master/scripts/notebooks/io_example.ipynb).\n",
    "\n",
    "Raw HERA visibility files for IDR2 blessed by the commissioning team have the format `zen.2458098.43869.HH.uvh5` which can be interpreted as:\n",
    "\n",
    "* `zen`: zenith pointing\n",
    "* `2458098`: Julian date integer\n",
    "* `43869`: first five decimal digits of the time of the first integration in the file \n",
    "* `HH`: \"HERA Hex\", visibilities purely between HERA core antennas\n",
    "* `uvh5`: file extension for `uvh5` files, a hdf5-based, `pyuvdata`-compliant data format for visibilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl = (25, 51, 'ee')\n",
    "sample_data_file = os.path.join(analysis_folder, raw_data_fn)\n",
    "# this is the baseline and file we examine through this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hera_cal.io import HERAData\n",
    "hd = HERAData(sample_data_file)\n",
    "print('This file has', len(hd.times), 'integrations', 'and', len(hd.freqs), 'frequency channels.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HERAData objects (when initialized with `uvh5` files) automatically load relevant metadata and store in the object, including `times`, `freqs`, `antpos`, `bls`, `pols`, and `lsts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, flags, nsamples = hd.read(bls=[bl]) # only loads a single bl; default loads all bls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `data`, `flags`, and `nsamples` loaded through `hera_cal.io` are stored in `DataContainer` objects, which act like dictionaries mapping baseline tuples in the `(ant1,ant2,pol)` format like `(65,71,'ee')` to waterfalls of shape `(Ntimes, Nfreqs)`. DataContainers support getting data and flags via `[]` and normal dictionary functions like `.keys()`, `.values()`, `.items()`, `.has_key()`. They also know to conjugate data when the reversed baseline key is provided and they abstract away polarization capitalization convetions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.all(data[25, 51, 'ee'] == np.conj(data[51, 25, 'ee'])))\n",
    "print(np.all(data[25, 51, 'EE'] == data[25, 51, 'ee']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This then allows for easy data access and plotting, such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,3), dpi=100)\n",
    "plt.imshow(np.angle(data[bl]) / (~flags[bl]), aspect='auto', cmap='twilight',\n",
    "           extent=[hd.freqs[0]/1e6, hd.freqs[-1]/1e6, hd.times[-1]-2458098, hd.times[0]-2458098])\n",
    "plt.ylabel('JD - 2458098')\n",
    "plt.xlabel('Frequency (MHz)')\n",
    "plt.title('Raw Visibilities: ' + str(bl))\n",
    "plt.colorbar(label='Phase (Radians)', aspect=8, pad=.025);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from hera_cal.apply_cal import calibrate_in_place\n",
    "from hera_cal.io import HERAData, HERACal\n",
    "from hera_cal.redcal import get_reds\n",
    "%matplotlib inline\n",
    "JD = 2458098.43869\n",
    "from matplotlib import rc\n",
    "rc('font',**{'family':'serif', 'serif':['cm']})\n",
    "rc('text', usetex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = '/lustre/aoc/projects/hera/H1C_IDR2/IDR2_2_pspec/v2/one_group/data/zen.grp1.of1.LST.1.31552.HH.OCRSLP2.uvh5'\n",
    "file2 = '/lustre/aoc/projects/hera/H1C_IDR2/IDR2_2_pspec/v2/one_group/data/zen.grp1.of1.LST.1.40949.HH.OCRSLP2.uvh5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hd_lstb1 = HERAData(file1)\n",
    "hd_lstb2 = HERAData(file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slct_redg = [(12, 13, 'ee'), (25, 26, 'ee'), (36, 37, 'ee')]\n",
    "lstb_data1, lstb_flags1, nsamples1 = hd_lstb1.read(bls=slct_redg)\n",
    "lstb_data2, lstb_flags2, nsamples2 = hd_lstb2.read(bls=slct_redg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstb_data = {k: np.concatenate((v[43:], lstb_data2[k][:13]), axis=0)  for k, v in lstb_data1.items()}\n",
    "lstb_flags = {k: np.concatenate((v[43:], lstb_flags2[k][:13]), axis=0)  for k, v in lstb_flags1.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reds(data, redbls, cbarlabel, flags=None, vcomp='phase', bad_bls=None, \\\n",
    "              ncol=1, style_ctxt='seaborn-white', figsize=(6, 6)):\n",
    "    \"\"\"Grid plot of visibilities at the different calibration stages\"\"\"\n",
    "    if vcomp == 'amp':\n",
    "        label = 'Amplitude'\n",
    "        vcalc = np.absolute\n",
    "        vmin = None\n",
    "        vmax = None\n",
    "    elif vcomp == 'phase':\n",
    "        label = 'Phase (Radians)'\n",
    "        vcalc = np.angle\n",
    "        vmin = -np.pi\n",
    "        vmax = np.pi\n",
    "    else:\n",
    "        raise ValueError('Specify either {\"amp\", \"phase\"} for vcomp.')\n",
    "\n",
    "    with plt.style.context((style_ctxt)): # use dark_background for white text\n",
    "        fig, axes = plt.subplots(int(np.ceil(len(redbls)/ncol)), ncol, sharex=True, \\\n",
    "                                 sharey=True, figsize =figsize, dpi=300)\n",
    "        for i, (bl, ax) in enumerate(zip(redbls, axes.flatten())):\n",
    "            # To make black subplot\n",
    "            if bad_bls is not None and bl in bad_bls:\n",
    "                vmin_ = np.pi\n",
    "            else:\n",
    "                vmin_ = vmin\n",
    "\n",
    "            data_bl = vcalc(data[bl])\n",
    "            if flags is not None:\n",
    "                data_bl_ = data_bl.copy()\n",
    "                data_bl_[flags[bl]] = vmin\n",
    "            else:\n",
    "                data_bl_ = data_bl\n",
    "\n",
    "            im = ax.imshow(data_bl_, cmap='inferno', aspect='auto', \\\n",
    "                           extent=[100, 200, 51, 0], vmin=vmin_, vmax=vmax, interpolation='nearest', rasterized=True)\n",
    "            ax.text(101-.1, 46-.3, str(bl), color='k', fontsize=20)\n",
    "            ax.text(101, 46, str(bl), color='w', fontsize=20)\n",
    "            if i >= len(axes.flatten()) - ncol:\n",
    "                ax.set_xlabel('Frequency (MHz)', size=18)\n",
    "            ax.set_yticks([])\n",
    "            ax.tick_params(labelsize=18)\n",
    "            # To get correct cbar range\n",
    "            if bad_bls is not None:\n",
    "                if bl not in bad_bls:\n",
    "                    cim = im\n",
    "            else:\n",
    "                cim = im\n",
    "        plt.tight_layout()\n",
    "        cbar = plt.colorbar(cim, ax=axes.ravel().tolist(), orientation='horizontal', \\\n",
    "                            label=cbarlabel, aspect=40)\n",
    "        cbar.ax.xaxis.label.set_font_properties(matplotlib.font_manager.\\\n",
    "                                                FontProperties(size=14))\n",
    "        cbar.ax.tick_params(labelsize=14)\n",
    "#         import os\n",
    "#         save_fig_dir = '/lustre/aoc/projects/hera/mmolnar/figs'\n",
    "#         plt.savefig(os.path.join(save_fig_dir, 'inp_vis.pdf'), bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reds(lstb_data, slct_redg, \\\n",
    "          'Phase of Raw Visibilities for Baselines Redundant to {}'.format(slct_redg[0]), \\\n",
    "          flags=lstb_flags, bad_bls=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating delay spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hera_pspec as hp\n",
    "from pyuvdata import UVData\n",
    "from pyuvdata import utils as uvutils\n",
    "plt.rcParams[\"figure.figsize\"] = (12,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load beam model\n",
    "beamfile = os.path.join(hera_pkgs, 'hera_pspec/hera_pspec/data/HERA_NF_dipole_power.beamfits')\n",
    "cosmo = hp.conversions.Cosmo_Conversions()\n",
    "uvb = hp.pspecbeam.PSpecBeamUV(beamfile, cosmo=cosmo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pspec_calc(dfile):\n",
    "\n",
    "    # Load data into UVData objects\n",
    "    uvd = UVData()\n",
    "    uvd.read_uvh5(dfile)\n",
    "    \n",
    "    # find conversion factor from Jy to mK\n",
    "    Jy_to_mK = uvb.Jy_to_mK(np.unique(uvd.freq_array), pol='xx')\n",
    "\n",
    "    # reshape to appropriately match a UVData.data_array object and multiply in!\n",
    "    uvd.data_array *= Jy_to_mK[None, None, :, None]\n",
    "\n",
    "    # We only have 1 data file here, so slide the time axis by one integration \n",
    "    # to avoid noise bias (not normally needed!)\n",
    "    uvd1 = uvd.select(times=np.unique(uvd.time_array)[:-1:2], inplace=False)\n",
    "    uvd2 = uvd.select(times=np.unique(uvd.time_array)[1::2], inplace=False)\n",
    "\n",
    "    # Create a new PSpecData object\n",
    "    ds = hp.PSpecData(dsets=[uvd1, uvd2], wgts=[None, None], beam=uvb)\n",
    "    # Here had to go into pyuvdata utils and change pol dicts s.t. xx->ee, yy->nn, xy->en, yx->ne etc.\n",
    "    # Will have to reinstall pyuvdata once this is fixed\n",
    "    \n",
    "    # Because we are forming power spectra between datasets that are offset in LST there will be some\n",
    "    # level of decoherence (and therefore signal loss) of the EoR signal. For short baselines and small\n",
    "    # LST offsets this is typically negligible, but it is still good to try to recover what coherency\n",
    "    # we can, simply by phasing (i.e. fringe-stopping) the datasets before forming the power spectra. \n",
    "    # This can be done with the rephase_to_dset method, and can only be done once.    \n",
    "    ds.rephase_to_dset(0) # Phase to the zeroth dataset\n",
    "    \n",
    "    # change units of UVData objects\n",
    "    ds.dsets[0].vis_units = 'mK'\n",
    "    ds.dsets[1].vis_units = 'mK'\n",
    "\n",
    "    # Find list of baselines pairs to calculate power spectra for\n",
    "    uvd_ant_copy = uvd.copy()\n",
    "    # uvd_ant_copy.unphase_to_drift(use_ant_pos=True)\n",
    "    tol = 1.0  # Tolerance in meters\n",
    "    uvd_ant_copy.select(times=uvd_ant_copy.time_array[0])\n",
    "\n",
    "    # Returned values: list of redundant groups, corresponding mean baseline vectors, baseline lengths. \n",
    "    # No conjugates included, so conjugates is None.\n",
    "    baseline_groups, vec_bin_centers, lengths = uvutils.get_baseline_redundancies(uvd_ant_copy.baseline_array, \\\n",
    "                                                                                  uvd_ant_copy.uvw_array, tol=tol)\n",
    "\n",
    "    # Selecting shortest (~14.6m) EW baselines group\n",
    "    if len(baseline_groups) == len([bl for bl_group in baseline_groups for bl in bl_group]):\n",
    "        # Check to see if baselines haven't already been aggregated by group - this is done in omnical\n",
    "        # where only 'only one baseline per unique separation' is kept\n",
    "        bls_to_include = baseline_groups[0]\n",
    "        bls1 = [uvutils.baseline_to_antnums(bls_to_include[0], len(uvd.get_ants()))]\n",
    "        bls2 = bls1\n",
    "        \n",
    "    else:\n",
    "        bls_to_include = baseline_groups[1]\n",
    "        \n",
    "        # Converting to antnum tuples to be used to construct_blpairs later on\n",
    "        # How do errors reduce when all cross-correlations are averaged?\n",
    "        ant_pairs_to_include = [uvutils.baseline_to_antnums(i, len(uvd.get_ants())) for i in bls_to_include]\n",
    "        bls1, bls2, blp = hp.utils.construct_blpairs(ant_pairs_to_include, exclude_permutations=False, exclude_auto_bls=True)\n",
    "\n",
    "    # Power spectrum calculation\n",
    "    uvp = ds.pspec(bls1, bls2, (0, 1), [('xx', 'xx')], spw_ranges=[(300, 400), (600,721)], input_data_weight='identity', norm='I', \n",
    "                   taper='blackman-harris', verbose=False)\n",
    "\n",
    "    blpairs = np.unique(uvp.blpair_array)\n",
    "    blps = list(blpairs)\n",
    "    \n",
    "    return uvp, blps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # example calculation on raw data\n",
    "# uvp, blps = pspec_calc(sample_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the spectra averaged over baseline-pairs and times\n",
    "# ax = hp.plot.delay_spectrum(uvp, [blps,], spw=0, pol=('xx','xx'), average_blpairs=True, average_times=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The UVData files contain 3 time bins; let's average over baseline-pairs but keep the time bins intact. \n",
    "# # We can also use the shorthand 'ee' to specify the matching polarization pair ('ee', 'ee')\n",
    "# ax = hp.plot.delay_spectrum(uvp, [blps,], spw=0, pol='xx', average_blpairs=True, average_times=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # And now let's try the opposite: average over times, but keep the baseline-pairs separate.\n",
    "# ax = hp.plot.delay_spectrum(uvp, [blps,], spw=0, pol='xx', average_blpairs=False, average_times=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Array During IDR2 and Bad Antennas \n",
    "\n",
    "The data taken for IDR2.2 began on 2458098 and ended on 2458116 with 52 antennas (~40 working well). Bad antennas, are summarized in this folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_ants_folder = os.path.join(hera_pkgs, 'hera_opm/pipelines/h1c/idr2/v2/bad_ants/')\n",
    "bad_ants = np.loadtxt(os.path.join(bad_ants_folder, '2458098.txt')).astype(int)\n",
    "print('Bad antennas on 2458098:', bad_ants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These bad antennas be include those identified by the commissioning team as bad as well as several that were found to be particularly non-redundant in an earlier round of analysis. We can visualize the array on 2458098 as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rc\n",
    "rc('font',**{'family':'serif', 'serif':['cm']})\n",
    "rc('text', usetex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pos = np.array(list(hd.antpos.values()))[:,0] - hd.antpos[1][0]  # subtract x pos of antenna 1\n",
    "y_pos = np.array(list(hd.antpos.values()))[:,1] - hd.antpos[65][1]\n",
    "ants = hd.antpos.keys()\n",
    "c = [ant in bad_ants for ant in ants]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idr2_jds = [2458098, 2458099, 2458101, 2458102, 2458103, 2458104, 2458105, \\\n",
    "            2458106, 2458107, 2458108, 2458109, 2458110, 2458111, 2458112, \\\n",
    "            2458113, 2458114, 2458115, 2458116]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1c_idr2_bad_ants = [np.loadtxt(os.path.join(bad_ants_folder, f'{jd}.txt')).astype(int) \\\n",
    "                                              for jd in idr2_jds]\n",
    "h1c_idr2_bad_ants = np.array(sorted(list(set.intersection(*map(set, h1c_idr2_bad_ants)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_ba = [2, 11, 24, 53, 54, 67, 69, 122, 139]\n",
    "met_ba = [0, 50, 98, 136]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = ['tomato' if ant in cal_ba else 'darkcyan' if ant in met_ba else 'white' for ant in ants]\n",
    "# c = ['orangered' if ant in h1c_idr2_bad_ants else 'white' for ant in ants]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_bool = [a in cal_ba for a in ants]\n",
    "met_bool = [a in met_ba for a in ants]\n",
    "ant_bool = [a not in h1c_idr2_bad_ants for a in ants]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6), dpi=100)\n",
    "ax.tick_params(labelsize=14, size=5)\n",
    "ax.scatter(x_pos, y_pos, c=c, s=1100, edgecolors='black')\n",
    "for i, (ant, pos) in enumerate(hd.antpos.items()):\n",
    "    if ant not in bad_ants:\n",
    "        ax.text(x_pos[i], y_pos[i], str(ant), va='center', ha='center', color='black', fontsize=12, weight='bold')\n",
    "    else:\n",
    "        ax.text(x_pos[i], y_pos[i], str(ant), va='center', ha='center', color='white', fontsize=12, weight='bold')\n",
    "\n",
    "classes = ['Calibration','Ant. Metrics']\n",
    "class_colours = ['tomato','darkcyan']\n",
    "# classes = ['Flagged']\n",
    "# class_colours = ['orangered']\n",
    "recs = []\n",
    "for i in range(len(class_colours)):\n",
    "    recs.append(mpatches.Rectangle((1000, 1000), 1, 1, fc=class_colours[i]))\n",
    "ax.legend(recs, classes, loc='lower left', prop={'size': 12})\n",
    "\n",
    "ax.set_xlabel(\"East-West [m]\", fontsize=14)\n",
    "ax.set_ylabel(\"North-South [m]\", fontsize=14)\n",
    "ax.set_title('HERA Phase I Array Layout', fontsize=14)\n",
    "ax.set_xlim(-80, 57)\n",
    "ax.set_ylim(-77, 65)\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# save_fig_dir = '/lustre/aoc/projects/hera/mmolnar/figs'\n",
    "# plt.savefig(os.path.join(save_fig_dir, 'h1c_antpos1.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All data in this IDR come from antennas in the Southwest sector of the split HERA core.\n",
    "\n",
    "Additionally, we inherit antenna metrics from the commissioning team. These come in json files with the format `zen.2458098.43869.HH.uv.ant_metrics.json`. These can be read `hera_qm.ant_metrics.load_antenna_metrics()`.\n",
    "\n",
    "The list of bad antennas generated by `ant_metrics` is generally a subset of the flagged antennas, but both are ORed together on a file-by-file basis. These antennas are left in all data/calibration files, but their flag arrays are all `True`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Overview\n",
    "\n",
    "The IDR 2.2 pipeline was run using [hera_opm](https://github.com/HERA-Team/hera_opm), which interfaces with the [Makeflow Workflow System](http://ccl.cse.nd.edu/software/makeflow/). The configuration files with the specific parameters used, associated bash scripts, and other files related bad antennas [can all be found here.](https://github.com/HERA-Team/hera_opm/tree/master/pipelines/h1c/idr2/v2)\n",
    "\n",
    "Below is a flowchart (which also lives in `hera_opm`) detailing all the analysis steps in IDR 2.2, including the interdependencies of the pipeline and the outputs at various steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://nbviewer.jupyter.org/github/HERA-Team/hera_pipelines/blob/main/'\n",
    "       'pipelines/h1c/idr2/v2/Analysis_Flowchart.pdf', 980, 1650)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mainline Day-by-Day Data Products\n",
    "\n",
    "The mainline daily analysis pipeline includes several steps of calibration, RFI-flagging, and an optional delay-filtering step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redcal: Redundant-Baseline Calibration\n",
    "\n",
    "Redundant-baseline calibration solves for antenna gains and visibility solutions in order to minimize the difference between the observations and the model visibilities, expressed in terms of $\\chi^2$:\n",
    "\n",
    "$\\chi^2(t,\\nu) = \\sum_{i,j} |V_{ij}^\\text{obs}(t,\\nu) - g_i(t,\\nu) g_j^*(t,\\nu) V_{i-j}^\\text{sol}(t,\\nu)|^2 / \\sigma^2_{ij}$.\n",
    "\n",
    "The idea of redundant calibration is to solve for $g_i$ and $V_{ij}^\\text{sol}$ simultaneously, taking advantage of the fact that there are far more total visibility measurements than unique baselines. This process takes place in three steps, `firstcal`, `logcal`, and `lincal` which iteratively searches for the minimum value of $\\chi^2$ by first restricting degrees of freedom (`firstcal` finds one delay and one phase offset per antenna), finding an approximate per-frequency calibration solution with `logcal`, and then refining that solution with `lincal`. \n",
    "\n",
    "This procedure operates on `ee` and `nn` visibilities independently, cross-polarized visibilities are not used. Each time, frequency, and polarization has four degeneracies that cannot be solved for with redundant calibration (because they do not affect $\\chi^2$):\n",
    "* Overall amplitude\n",
    "* Overall phase\n",
    "* North-South phase gradient\n",
    "* East-West phase gradient\n",
    "\n",
    "For a more detailed, pedagogical explanation of the degeneracies of redundant-baseline calibration, see [Section 2 of this paper.](https://arxiv.org/abs/1712.07212). A thorough overview of the results from redundant calibration of HERA data in IDR 2.2 is forthcoming in Dillon et al. *(in prep.)*.\n",
    "\n",
    "**New in IDR 2.2:** \n",
    "* `firstcal` has been rewritten to use the `linsolve` package and to iteratively solve for phase offsets (as opposed to just delays).  \n",
    "* `lincal` no longer uses the `omnical` package but rather a pure python implementation of it in `hera_cal.redcal`, which is both faster and more accurate (see [HERA Memo #50](http://reionization.org/wp-content/uploads/2013/03/HERA_memo050_Omnical-Convergence.pdf)). \n",
    "* $\\chi^2 / DoF$ is now properly normalized (see [HERA Memo #61](http://reionization.org/wp-content/uploads/2019/01/HERA061_Omnical_Chisq_per_DoF.pdf)). \n",
    "* The omnical visibility solutions are subsequently updated to include absolute calibration and flagging. See below.\n",
    "\n",
    "As the above diagram indicates, the output of redundant calibration includes `firstcal` calibration solutions, `omnical` calibration solutions, and `omnical` visibility solutions for unique baselines.  The latter is output as a `.uvh5` file, albeit with fewer baselines than the full data (only one per unique separation). The first two are saved as `.calfits` files. Calibration solution in `.calfits` files can be read with `pyuvdata` or using the `HERACal` subclass in an analogous way to `HERAData`. `HERACal`'s `.read()` function returns gains, flags, and $\\chi^2$ per antenna, and overall $\\chi^2$. In the `omnical` calibration solutions, that $\\chi^2$ is a properly normalized $\\chi^2/DOF$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hera_cal.io import HERACal\n",
    "from hera_cal.utils import split_bl\n",
    "ant = split_bl(bl)[0]\n",
    "\n",
    "# load firstcal gains and flags\n",
    "firstcal_file = os.path.join(analysis_folder, 'zen.{}.HH.first.calfits'.format(JD))\n",
    "hc = HERACal(firstcal_file)\n",
    "fc_gains, fc_flags, _, _ = hc.read()\n",
    "\n",
    "# load redundant calibration gains, flags, chi^2 per antenna, and chi^2 / DoF\n",
    "omnical_file = os.path.join(analysis_folder, 'zen.{}.HH.omni.calfits'.format(JD))\n",
    "hc = HERACal(omnical_file)\n",
    "oc_gains, oc_flags, oc_quals, oc_total_quals = hc.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that relevant information about the files produced (git hashes, command line invocation, etc.) by the pipeline is available in their histories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def waterfall_column(waterfalls, flags, titles, clims=None, clabels=None, cmaps=None, ylims=None, \n",
    "                     ylabel='JD - 2458098', extents=None, hspace=.1, figsize=(12,6), dpi=100):\n",
    "    '''Useful plotting function for the IDR 2.2 memo.'''\n",
    "    if clims is None:\n",
    "        clims = [None for i in range(len(waterfalls))]\n",
    "    if clabels is None:\n",
    "        clabels = [None for i in range(len(waterfalls))]\n",
    "    if cmaps is None:\n",
    "        cmaps = [None for i in range(len(waterfalls))]\n",
    "    if ylims is None:\n",
    "        ylims = [None for i in range(len(waterfalls))]\n",
    "    if not any(isinstance(ex, list) for ex in extents):\n",
    "        extents = [extents for i in range(len(waterfalls))]\n",
    "\n",
    "    fig, axes = plt.subplots(len(waterfalls), 1, sharex=True, squeeze=True, figsize=figsize, dpi=dpi)\n",
    "    plt.subplots_adjust(hspace=hspace)\n",
    "    for ax, wf, f, t, clim, clabel, cmap, ylim, ex in zip(axes, waterfalls, flags, titles,\n",
    "                                                          clims, clabels, cmaps, ylims, extents):\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            im = ax.imshow(wf / ~f, aspect='auto', extent=ex, cmap=cmap)\n",
    "        plt.colorbar(im, ax=ax, label=clabel, aspect=8, pad=.025)\n",
    "        if ax == axes[-1]:\n",
    "            ax.set_xlabel('Frequency (MHz)')    \n",
    "        im.set_clim(clim)\n",
    "        ax.set_ylabel(ylabel)\n",
    "        ax.set_ylim(ylim)\n",
    "        props = dict(boxstyle='round', facecolor='white', alpha=0.8)\n",
    "        ax.text(0.02, 0.9, t, transform=ax.transAxes, fontsize=14, verticalalignment='top', bbox=props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waterfall_column([np.angle(fc_gains[ant]), np.angle(oc_gains[ant]), oc_total_quals[ant[1]]],\n",
    "                 [fc_flags[ant], oc_flags[ant], oc_flags[ant]],\n",
    "                 ['Firstcal: {}'.format(ant), 'Omnical: {}'.format(ant), 'Omnical $\\chi^2$ / DOF'],\n",
    "                 clims=[None, None, [0, 3]],\n",
    "                 clabels=['Phase (Radians)', 'Phase (Radians)', '$\\chi^2$ / DoF (Unitless)'],\n",
    "                 cmaps = ['twilight', 'twilight', 'inferno'],\n",
    "                 extents=[hc.freqs[0] / 1e6, hc.freqs[-1] / 1e6, hc.times[-1] - 2458098, hc.times[0] - 2458098])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see redundant baseline calibration solutions, which are dominated by an overall delay. We also see that $\\chi^2 / DoF$ is generically greater than 1, indicating non-redundancy that seems to vary both in frequency and time. We also see clear evidence of narrow-band RFI which has not yet been excised. The first and last 50 channels were flagged by hand, a few more channels also got flagged at the high end of the band for producing `nan`s in the calibration solution, which usually occurs when there are 0s in the data. At this stage, we also flag any data where the sun is above the horizon, which explains why the IDR contains many completely flagged files. While some of the integrations appear discrepant, especially after `firstcal`, this turns out to be discontinuities in the degenerate subspace that are subsequently fixed by `abscal`. \n",
    "\n",
    "Next, we compare redundantly calibrated data to the visibility solution for all redundant baselines in the same group. To do that, we use `hera_cal.apply_cal.calibrate_in_place()` to calibrate the data we've already loaded above and we use `hera_cal.redcal.get_reds()` to convert antenna positions into a list of redundant baseline groups, which are themselves lists of baselines. The first baseline in the group is used as the key in the omnical visibility solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply calibration solution to data\n",
    "from copy import deepcopy\n",
    "from hera_cal.apply_cal import calibrate_in_place\n",
    "redcal_data, redcal_flags = deepcopy(data), deepcopy(flags)\n",
    "calibrate_in_place(redcal_data, oc_gains, data_flags=redcal_flags, cal_flags=oc_flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the mapping of baselines to the unique baseline key used in the omnical visibility solutions \n",
    "from hera_cal.redcal import get_reds\n",
    "reds = get_reds(hd.antpos, pols=['ee'])\n",
    "red_dict = {bl: bl_group[0] for bl_group in reds for bl in bl_group} \n",
    "omnical_vis_file = os.path.join(analysis_folder, 'zen.{}.HH.omni_vis.uvh5'.format(JD))\n",
    "hd_oc = HERAData(omnical_vis_file)\n",
    "omnivis, omnivis_flags, _ = hd_oc.read(bls=[red_dict[bl]]) # note, this only has the first baseline in each redundant group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waterfall_column([np.angle(redcal_data[bl]), np.angle(omnivis[red_dict[bl]])],\n",
    "                 [redcal_flags[bl], omnivis_flags[red_dict[bl]]],\n",
    "                 ['Redundantly Calibrated: {}'.format(bl), 'Omnical Visibility Solution: {}'.format(bl)],\n",
    "                 clabels=['Phase (Radians)', 'Phase (Radians)'],\n",
    "                 cmaps = ['twilight', 'twilight'],                 \n",
    "                 extents=[hd.freqs[0] / 1e6, hd.freqs[-1] / 1e6, hd.times[-1] - 2458098, hd.times[0] - 2458098])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much of the difference between the individual calibrated visibility and the visibility solution is noise; the visibility solution is approximately equal to the average of all redundant baselines in a group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delay Spectra "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uvp, blps = pspec_calc(omnical_vis_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the spectra averaged over baseline-pairs and times\n",
    "# # Here we are plotting the auto power-spectrum with baselines of type (0, 1), since for the omni visibility\n",
    "# # file, the visibilities are already aggregated by baseline type\n",
    "# ax = hp.plot.delay_spectrum(uvp, [blps,], spw=0, pol=('xx','xx'), average_blpairs=False, average_times=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abscal: Fixing Redcal Degeneracies with Externally Calibrated Visibilities\n",
    "\n",
    "The next step in calibration is absolute calibration or `abscal`. Absolute calibration attaches the `omnical` gain solutions to a sky reference, allowing us to fix the degeneracies. In fact, absolute calibration is restricted to only modify the degenerate subspace of the omnical solutions; only four degrees of freedom per polarization per time per frequency are allowed to be modified.\n",
    "\n",
    "Instead of using a point source model, `abscal` takes as its input a set of externally calibrated visibilities which we call the \"abscal model,\" though simulated visibilities could also work. Individual integrations in the model are phased to match the nearest integration in the data. As in redundant calibration, `abscal` first calibrates a delay and phase slope across the array (a subset of the degeneracies) before solving for the degeneracies directly, namely a per-frequency and per-time phase slope and amplitude. \n",
    "\n",
    "The overall phase degeneracy cannot be solved by `abscal` because it has no impact on calibrated visibilities. To resolve it, we pick a reference antenna and rephase all other gains so that it has zero phase for all frequencies and times. We choose a reference antenna per-file and per-polarization that 1) has no more flags than any other antenna and 2) introduces the least phase noise into the other gains. Later in the calibration smoothing step, we will rephase to a single reference antenna for the whole night using the same criteria. \n",
    "\n",
    "Because polarizations are calibrated separately without reference to any $en$- or $ne$-visibilities, we have no reason to expect that the cross-polarized visibilties are properly calibrated. This does not affect our analysis of psuedo-Stokes I or Q images or power spectra, but it does mean that our pseudo-Stokes U and V images and power spectra are not reliable at this point.\n",
    "\n",
    "**New in IDR 2.2**:\n",
    "\n",
    "In IDR 2.2, we added a notion of `abscal` $\\chi^2$, defined as:\n",
    "\n",
    "$\\chi^2_\\text{abs}(t,\\nu) = \\sum_{i,j} |V_{ij}^\\text{model}(t,\\nu) - g^\\text{abs}_i(t,\\nu) g^{*\\text{abs}}_j(t,\\nu) V_{i,j}^\\text{raw}(t,\\nu)|^2 / \\sigma^2_{ij}$,\n",
    "\n",
    "where $\\sigma^2$ is the noise on the absolute calibrated visibilities (as inferred from the autocorrelations). Unlike the `omnical` $\\chi^2$, this is not properly normalized per DoF. However, it is still a useful metric of discrepancy between the data after `redcal` and `abscal` and the externally calibrated visibilities. \n",
    "\n",
    "For IDR2.2, the data was calibrated by looking at three days that span the LST range of what will eventually be H1C IDR3, namely 2458042, 2458116, and 2458161. Each day was calibrated in CASA using a different calibrator field from [GLEAM](http://www.mwatelescope.org/gleam) and only baselines > 40 m in length:\n",
    "* 2458042 was calibrated on a field centered at 2.0167 hours of LST (including GLEAM J0200-3053)\n",
    "* 2458116 was calibrated on a field centered at 5.2367 hours of LST (including GLEAM J0513-3028)\n",
    "* 2458161 was calibrated on a field centered at 14.4428 hours of LST (including GLEAM J1425-2959)\n",
    "\n",
    "The calibration solutions were assumed to be stable over the course of each calibration day. Then, to form one synthetic day externally calibrated visibilities, these days are \"cross-faded\" into each other, effectively a weighted average of the three days with weights that fall linearly away from the calibration field and are 0 when the sun is up. \n",
    "\n",
    "Finally, the abscal model was low-pass delay filtered (using a [Tukey window](https://en.wikipedia.org/wiki/Window_function#Tukey_window) with $\\alpha=0.2$) at either the baseline's horizon + 50 ns, or 150 ns, whichever is longer. This lowered the noise on the model, removed any high-frequency spectral structure we don't expect to be associated with the sky, and allowed interpolation over RFI-contaminated channels. Unfortunately, this also risks spreading out any isolated but unflagged RFI, which we see some evidence of in the the abscal $\\chi^2$ below. \n",
    "\n",
    "For more on absolute calibration, see [HERA Memo #42](http://reionization.org/wp-content/uploads/2013/03/abscal_memo.pdf) for the IDR 2.1 implementation or Kern et al. *(in prep.)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load abscal calibration solutions\n",
    "abscal_file = os.path.join(analysis_folder, 'zen.{}.HH.abs.calfits'.format(JD))\n",
    "hc = HERACal(abscal_file)\n",
    "ac_gains, ac_flags, ac_quals, ac_total_quals = hc.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "waterfall_column([np.abs(ac_gains[(ant)]), np.angle(ac_gains[ant]), ac_total_quals[ant[1]]],\n",
    "                 [ac_flags[ant], ac_flags[ant], ac_flags[ant]],\n",
    "                 ['Abscal Gain Amplitude: {}'.format(ant), 'Abscal Gain Phase: {}'.format(ant), 'Abscal $\\chi^2$'],\n",
    "#                  clims=[[0, .04], None, [0, 5]],\n",
    "                 clabels=['Amplitude (Unitless)', 'Phase (Radians)', '$\\chi^2$ (Unnormalized)'],\n",
    "                 cmaps = ['inferno', 'twilight', 'inferno'],\n",
    "                 extents=[hc.freqs[0] / 1e6, hc.freqs[-1] / 1e6, hc.times[-1] - 2458098, hc.times[0] - 2458098])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see that the absolutely calibrated data now closely match the reference at the same LSTs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply abscal calibration to data\n",
    "abscal_data, abscal_flags = deepcopy(data), deepcopy(flags)\n",
    "calibrate_in_place(abscal_data, ac_gains, data_flags=abscal_flags, cal_flags=ac_flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find corresponding LSTs in the externally calibrated Abscal Model\n",
    "import glob\n",
    "\n",
    "from hera_cal.abscal import match_times\n",
    "\n",
    "if local_comp:\n",
    "    model_dir = analysis_folder # local copy\n",
    "else:\n",
    "    model_dir = '/lustre/aoc/projects/hera/nkern/idr3_abscal_models/full_model/' # at NRAO\n",
    "    \n",
    "model_files = glob.glob(os.path.join(model_dir, 'zen.2458042.*.HH.uvRXLS.uvh5')) # cheatingly slightly by only checking the .5* files\n",
    "\n",
    "if not local_comp:\n",
    "    model_files = sorted(set(match_times(sample_data_file, model_files, filetype='uvh5')))\n",
    "print(model_files)\n",
    "hdm = HERAData(model_files)\n",
    "model, model_flags, _ = hdm.read(bls=[bl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# compare redundantly and then absolutely calibrated data to externally calibrated reference\n",
    "waterfall_column([np.angle(abscal_data[bl]), np.angle(model[bl])],\n",
    "                 [abscal_flags[bl], model_flags[bl]],\n",
    "                 ['Absolutely Calibrated: {}'.format(bl), 'Externally Calibrated Reference: {}'.format(bl)],\n",
    "                 clabels=['Phase (Radians)', 'Phase (Radians)'],\n",
    "                 ylabel='LST (Radians)',\n",
    "                 cmaps = ['twilight', 'twilight'],                 \n",
    "                 ylims = [[model.lsts[-1], model.lsts[0]], None],                 \n",
    "                 extents=[[hd.freqs[0] / 1e6, hd.freqs[-1] / 1e6, hd.lsts[-1], hd.lsts[0]], \n",
    "                          [model.freqs[0] / 1e6, model.freqs[-1] / 1e6, model.lsts[-1], model.lsts[0]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFI Flagging\n",
    "\n",
    "Our Radio Frequency Interference (RFI) flagging algorithm has largely been overhauled for IDR2.2, though it still relies fundamentally on the basic idea of looking for highly-significant outliers in detrended data and then also flagging less significant outliers that neighbor them in time or frequency, the so-called \"watershed\" algorithm. This relies on the idea that RFI events are compact in frequency or time or both.\n",
    "\n",
    "**New in IDR 2.2:**\n",
    "\n",
    "The following algorithm is used for RFI flagging on a per-observation basis.\n",
    "\n",
    "1. *Compute metric waterfalls of calibration data products using median filters.* We begin by flagging on the `omnical` gains, `omnical` $\\chi^2/DoF$, the `omnical` visibility solutions, the `abscal` gains, and the `abscal` $\\chi^2$. Each baseline, antenna, or polarization's waterfall is median filtered (i.e. the local median subtracted off in a 8 integration by 8 channel box) and each each data point is compared to the local median and median absolute deviation to compute a metric of \"outlierness\" in number of $\\sigma$s. These \"metrics\" are averaged in quadrature to produce a single waterfall for each data product. \n",
    "\n",
    "\n",
    "2. *Flag and watershed on metrics waterfalls individually and collectively to produce a set of initial flags.* $5\\sigma$ outliers on each waterfall are flagged, and then any $2\\sigma$ outliers that neighbor a flag are also flagged. This process is performed both on individual calibration data products to look for specific issues with the associated step, and then on a equally-weighted average in quadrature of all 5 waterfalls to look for low level RFI. These are all ORed together to produce an initial set of flags.\n",
    "\n",
    "\n",
    "3. *Use the initial flags to compute metric waterfalls of both calibration products and raw data using mean filters.* The process is repeated on the five calibration data products above, but now also on the raw data itself (these were excluded from the first step because they are slow to work with). However, because the data now already have flags, we use a *mean* filter instead of a median filter, which can better handle setting the weight of flagged data to zero. Since the data are already pre-flagged, the worst outliers that would throw off this calculation are already gone. This is effectively computing a number of $\\sigma$s for each time and frequency in terms of a weighted local standard deviation. These are then averaged in quadrature to form metrics waterfalls. \n",
    "\n",
    "\n",
    "4. *Flag and watershed on new metrics waterfalls individually and collectively and OR those flags with the initial flags.*  Once again, we look for significant outliers ($> 5\\sigma$) and minor neighboring outliers (growing to encompass all $> 2\\sigma$ outliers that touch $> 5\\sigma$ outliers or each other) and flag both on individual metrics waterfalls and on the quadrature average of all six waterfalls. All the resulting flags are ORed together with the initial flags. \n",
    "\n",
    "\n",
    "5. *Perform day-long thresholding and broadcasting of flags.* The metrics waterfalls used for flagging individual frequencies and times are subsequently analyzed for an entire day. We examine median metrics across frequency or across time, renormalize, and look for significant outliers (in this case $> 7\\sigma$) or neighbors ($ >3\\sigma$) to flag for all times or frequencies. This is performed on all the different metrics used above and also ORed together with the previous flagging.\n",
    "\n",
    "\n",
    "6. *Update and save flags in updated `.flagged_abs.calfits` file.* Also save various intermediate metrics and flagging products as HDF5 files (readable with `pyuvdata.UVFlag`).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load flagged calibration solutions\n",
    "flagged_abscal_file = os.path.join(analysis_folder, 'zen.{}.HH.flagged_abs.calfits'.format(JD))\n",
    "hc = HERACal(flagged_abscal_file)\n",
    "fac_gains, fac_flags, fac_quals, fac_total_quals = hc.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waterfall_column([np.abs(fac_gains[ant]), np.angle(fac_gains[ant]), fac_total_quals[ant[1]]],\n",
    "                 [fac_flags[ant], fac_flags[ant], fac_flags[ant]],\n",
    "                 ['Flagged Abscal Amplitude: {}'.format(ant), \n",
    "                  'Flagged Abscal Phase: {}'.format(ant), \n",
    "                  'Flagged Abscal $\\chi^2$'],\n",
    "                 clims=[[0, .04], None, [0, 5]],\n",
    "                 clabels=['Amplitude (Unitless)', 'Phase (Radians)', '$\\chi^2$ (Unnormalized)'],\n",
    "                 cmaps = ['inferno', 'twilight', 'inferno'],\n",
    "                 extents=[hc.freqs[0] / 1e6, hc.freqs[-1] / 1e6, hc.times[-1] - 2458098, hc.times[0] - 2458098])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm seems to catch all obvious RFI and also flags the impact of RFI in the `abscal` model (as seen in the `abscal` $\\chi^2$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration Smoothing\n",
    "\n",
    "To reduce the number of degrees of freedom in the calibration solution, we rely on the assumption that the instrument does not have spectral structure on small frequency scales and that it does not evolve quickly in time. Our smoothing algorithm loads in a whole day of calibration solutions simultaneously and put them on a time grid, leaving flagged holes for any missing files or missing time.\n",
    "\n",
    "**New in IDR 2.2:**\n",
    "\n",
    "Most notably, we have implemented 2D smoothing. In IDR 2.1, we smoothed in time and then in frequency. Here we do both simultaneously using a 2D CLEANing algorithm. We restrict our CLEANed solution in delay/fringe-rate space,  filtering on 10 MHz (i.e. 100 ns) and 30 minute (0.556 mHz) scales. This filtering is done with a Tukey window with $\\alpha = .5$ in frequency and a top-hat window in time.\n",
    "\n",
    "The calibration smoothing step also checks for antennas that are often flagged as bad while others are not. We count the number of unflagged visibilities on each antenna and any antenna with less than 50% of the maximum among all antennas is flagged before smoothing is performed.\n",
    "\n",
    "Here is an example of a calibration smoothing in action: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_abscal_file = os.path.join(analysis_folder, 'zen.{}.HH.smooth_abs.calfits'.format(JD))\n",
    "hc = HERACal(smoothed_abscal_file)\n",
    "sac_gains, sac_flags, _, _ = hc.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find antenna that was picked as the reference by looking for unflagged antennas with 0 phase\n",
    "sc_refant = {pol: sorted([(np.mean(np.abs(np.angle(sac_gains[ant]))), ant) \n",
    "                          for ant in sac_gains if ant[1]==pol and not np.all(sac_flags[ant])])[0][1]\n",
    "                          for pol in hc.pols}\n",
    "for pol in hc.pols:\n",
    "    print('Antenna {} chosen as the reference for {}.'.format(sc_refant[pol][0], pol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rephase abscal to to have the same reference antenna as smooth_cal\n",
    "rephased_abscal = fac_gains[ant] * np.abs(fac_gains[sc_refant[ant[1]]]) / fac_gains[sc_refant[ant[1]]]\n",
    "smoothcal_gains = deepcopy(sac_gains[ant])\n",
    "\n",
    "waterfall_column([np.abs(rephased_abscal), np.abs(smoothcal_gains)],\n",
    "                 [fac_flags[ant], sac_flags[ant]],\n",
    "                 ['Abscal Amplitude: {}'.format(ant), 'Smoothcal Amplitude: {}'.format(ant)],\n",
    "                 clabels=['Amplitude (Unitless)', 'Amplitude (Unitless)'],\n",
    "                 cmaps = ['inferno', 'inferno'],\n",
    "                 figsize=(12,4),\n",
    "                 clims=[[0, .04], [0, .04]],\n",
    "                 extents=[hc.freqs[0] / 1e6, hc.freqs[-1] / 1e6, hc.times[-1] - 2458098, hc.times[0] - 2458098])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace flags with nans in order to take a nanmedian over time\n",
    "rephased_abscal[fac_flags[ant]] = np.nan\n",
    "smoothcal_gains[sac_flags[ant]] = np.nan\n",
    "\n",
    "# plot median smooth calibration\n",
    "plt.figure(figsize=(12,3), dpi=100)\n",
    "plt.plot(hc.freqs / 1e6, np.nanmedian(rephased_abscal, axis=0).real, '.', label='Flagged Abscal')\n",
    "plt.plot(hc.freqs / 1e6, np.nanmedian(smoothcal_gains, axis=0).real, label='Smoothcal')\n",
    "plt.title('Median Abscal and Smoothcal Gains')\n",
    "plt.ylabel('Real Part of Gains (Unitless)')\n",
    "plt.xlabel('Frequency (MHz)')\n",
    "plt.ylim(-.05, .05)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delay Filter\n",
    "\n",
    "At this point in the pipeline, we are ready to LST-bin our calibrated data. However, we also produce calibrated and delay-filtered data. These files, the `.OCRSD.uvh5` files are the only full-sized data files produced before LST-binning. `OCRSD` is a legacy nomenclature meaning <b>O</b>micaled, abs<b>C</b>aled, x<b>R</b>fi, <b>S</b>moothcaled, <b>D</b>elay-filtered data. \n",
    "\n",
    "Delay filtering is performed independently for each integration and each baseline in the calibrated and flagged data. Delay filtering removes Fourier content outside the baseline's horizon delay, plus a 15 ns buffer. To account for RFI gaps, it is done using the CLEAN algorithm with a $10^{-9}$ tolerance and a Tukey window with $\\alpha=0.5$. \n",
    "\n",
    "This procedure is largely the same as in IDR2.1 (the only significant update was improved memory management using partial I/O enabled by `.uvh5`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # zen.???????.?????.HH.OCRSD.uvh5 files removed from H1C_IDR2.2 - comment out\n",
    "\n",
    "# delay_filtered_file = os.path.join(analysis_folder, 'zen.{}.HH.OCRSD.uvh5'.format(JD))\n",
    "# hd_dfil = HERAData(delay_filtered_file)\n",
    "# dfil_data, dfil_flags, _ = hd_dfil.read(bls=[bl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply smooth_abs calibration to data\n",
    "# sac_data, sac_data_flags = deepcopy(data), deepcopy(flags)\n",
    "# calibrate_in_place(sac_data, sac_gains, data_flags=sac_data_flags, cal_flags=sac_flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# waterfall_column([np.abs(sac_data[bl]), np.abs(dfil_data[bl]), np.angle(sac_data[bl]), np.angle(dfil_data[bl])],\n",
    "#                  [sac_data_flags[bl], dfil_flags[bl], sac_data_flags[bl], dfil_flags[bl]],\n",
    "#                  ['Calibrated Amplitude: {}'.format(bl), 'Delay-Filtered Amplitude: {}'.format(bl),\n",
    "#                  'Calibrated Phase: {}'.format(bl), 'Delay-Filtered Phase: {}'.format(bl)],\n",
    "#                  clabels=['Amplitude (Jy)', 'Amplitude (Jy)', 'Phase (Radians)', 'Phase (Radians)'],\n",
    "#                  clims=[[0, 150], [0, 15], None, None],\n",
    "#                  cmaps = ['inferno', 'inferno', 'twilight', 'twilight'],\n",
    "#                  extents=[hd_dfil.freqs[0] / 1e6, hd_dfil.freqs[-1] / 1e6, \n",
    "#                           hd_dfil.times[-1] - 2458098, hd_dfil.times[0] - 2458098],\n",
    "#                  figsize=(12,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note the different color scale on the delay-filtered amplitude.)\n",
    "\n",
    "While the delay filter vastly reduces the amplitude of the visibility to much closer to the noise level, the result is not purely noise-like, as can be seen in the spectral structure in the delay-filtered phase. This may be, in part, due to some of the non-fringing structure and the cable reflections, both examined in the two Kern et al. (*submitted*) papers examining systematics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ancillary Day-by-Day Data Products\n",
    "\n",
    "The main goal of analysis is to calibrate and RFI-flag to as to enable LST-binning and the resultant reduction in data volume. However, we perform several additional steps that produce useful data products for quality assessment of the data and various open research questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update `omnical` Visibility Solutions with Absolute Calibration and Flagging\n",
    "\n",
    "**New in IDR 2.2:**\n",
    "\n",
    "Previously, `omnical` visibility solutions we only used to look for RFI. Now, we update the omnical visibilities by filling in their degenerate components using either the results of `abscal` or the results of `smooth_cal` (we do both and leave it as an open question whether there's any advantage or one or the other). This step is performed by taking the average of the ratio of `omnical` gain products to `abscal`/`smooth_cal` gain products in a redundant baseline group. This isolates the degenerate subspace, which we then use to update the visibility solutions. Like the `omnical` visibility solutions, these `.uvh5` data products contain one visibility per unique baseline group and an updated `nsamples`.\n",
    "\n",
    "While we do no subsequent processing of these data, they are meant to help enable research questions about whether this data reductions step (which is substantial for HERA) can help us get to the power spectrum quicker without losing information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load representative baseline from updated omnical visibility solutions\n",
    "fac_vis_file = os.path.join(analysis_folder, 'zen.{}.HH.flagged_abs_vis.uvh5'.format(JD))\n",
    "hd_fac = HERAData(fac_vis_file)\n",
    "facvis, facvis_flags, _ = hd_fac.read(bls=[red_dict[bl]])\n",
    "\n",
    "sac_vis_file = os.path.join(analysis_folder, 'zen.{}.HH.smooth_abs_vis.uvh5'.format(JD))\n",
    "hd_sac = HERAData(sac_vis_file)\n",
    "sacvis, sacvis_flags, _ = hd_sac.read(bls=[red_dict[bl]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waterfall_column([np.angle(abscal_data[bl]), np.angle(facvis[red_dict[bl]]), np.angle(sacvis[red_dict[bl]])],\n",
    "                 [sacvis_flags[red_dict[bl]], sacvis_flags[red_dict[bl]], sacvis_flags[red_dict[bl]]],\n",
    "                 ['Absolutely Calibrated and Flagged: {}'.format(bl), \n",
    "                  'Omnical Solution Updated with Flagged Abscal: {}'.format(bl),\n",
    "                  'Omnical Solution Updated with Smoothed Abscal: {}'.format(bl)],\n",
    "                 clabels=['Phase (Radians)', 'Phase (Radians)', 'Phase (Radians)'],\n",
    "                 cmaps = ['twilight', 'twilight', 'twilight'],                 \n",
    "                 extents=[hd.freqs[0] / 1e6, hd.freqs[-1] / 1e6, hd.times[-1] - 2458098, hd.times[0] - 2458098])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waterfall_column([np.abs(abscal_data[bl]), np.abs(facvis[red_dict[bl]]), np.abs(sacvis[red_dict[bl]])],\n",
    "                 [sacvis_flags[red_dict[bl]], sacvis_flags[red_dict[bl]], sacvis_flags[red_dict[bl]]],\n",
    "                 ['Absolutely Calibrated and Flagged: {}'.format(bl), \n",
    "                  'Omnical Solution Updated with Flagged Abscal: {}'.format(bl),\n",
    "                  'Omnical Solution Updated with Smoothed Abscal: {}'.format(bl)],\n",
    "                 clabels=['Amplitude (Jy)', 'Amplitude (Jy)', 'Amplitude (Jy)'],\n",
    "                 cmaps = ['inferno', 'inferno', 'inferno'],                 \n",
    "                 extents=[hd.freqs[0] / 1e6, hd.freqs[-1] / 1e6, hd.times[-1] - 2458098, hd.times[0] - 2458098])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Autocorrelations and Noise\n",
    "\n",
    "**New in IDR 2.2:**\n",
    "\n",
    "To enable future calibration tests, the pipeline extracts autocorrelations. While in principle this is duplicating data and is otherwise trivial with partial I/O, this step anticipates IDR 3.1 and beyond where we cannot keep the full data set staged on `lustre`. For future tests of calirabtion and noise, it [was requested](https://github.com/HERA-Team/hera_cal/issues/405) to store these raw data products alongside calibration solutions. These autocorrelations are stored in `.autos.uvh5` and only have autocorrelation baseline keys.\n",
    "\n",
    "Using calibrated autocorrelations, we can predict the noise variance on visibilities, $\\sigma_{ij}^2$. Namely,\n",
    "\n",
    "$\\sigma_{ij}^2 = V_{ii} V_{jj}$ $ / $ $B t$\n",
    "\n",
    "where $B$ is the bandwidth of a channel and $t$ is the integration time. Instead of computing this quantity for all baselines, we instead compute and save $\\sigma_{ii}$ where\n",
    "\n",
    "$\\sigma_{ij} \\equiv \\sqrt{\\sigma_{ii} \\sigma_{jj}} = \\left(V_{ii} / \\sqrt{Bt}\\right) \\left( V_{jj} / \\sqrt{Bt} \\right)$.\n",
    "\n",
    "These quantities, $\\sigma_{ii}$, are stored in `.noise_std.uvh5` files. Though they are technically per-antenna, we felt it more sensible to store them as visibility data files (since the units are Jy) with autocorrelation keys instead of storing them in `.calfits` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hera_cal.utils import join_bl\n",
    "ant1, ant2 = split_bl(bl)\n",
    "auto_bl1 = join_bl(ant1, ant1)\n",
    "auto_bl2 = join_bl(ant2, ant2)\n",
    "\n",
    "# Load autocorrelation\n",
    "autos_file = os.path.join(analysis_folder, 'zen.{}.HH.autos.uvh5'.format(JD))\n",
    "hd_autos = HERAData(autos_file)\n",
    "autos, auto_flags, _  = hd_autos.read(bls=[auto_bl1, auto_bl2])\n",
    "\n",
    "# Calibrate autocorrelation\n",
    "cal_autos, cal_auto_flags = deepcopy(autos), deepcopy(auto_flags)\n",
    "calibrate_in_place(cal_autos, sac_gains, data_flags=cal_auto_flags, cal_flags=sac_flags)\n",
    "\n",
    "# Load inferred noise on calibrated data\n",
    "noise_file = os.path.join(analysis_folder, 'zen.{}.HH.noise_std.uvh5'.format(JD))\n",
    "hd_noise = HERAData(noise_file)\n",
    "noise, noise_flags, _  = hd_noise.read(bls=[auto_bl1, auto_bl2])\n",
    "bl_noise = np.sqrt(noise[auto_bl1] * noise[auto_bl2])\n",
    "bl_noise_flags = noise_flags[auto_bl1] | noise_flags[auto_bl2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waterfall_column([np.abs(autos[auto_bl1]), np.abs(cal_autos[auto_bl1]), \n",
    "                  np.abs(noise[auto_bl1]), np.abs(bl_noise)],\n",
    "                 [auto_flags[auto_bl1], cal_auto_flags[auto_bl1], \n",
    "                  noise_flags[auto_bl1], bl_noise_flags],\n",
    "                 ['Raw Autocorrelations: {}'.format(auto_bl1), \n",
    "                  'Smooth-Calibrated Autocorrelations: {}'.format(auto_bl1),\n",
    "                  'Inferred Antenna Noise: {}.'.format(ant1),\n",
    "                  'Inferred Visibility Noise: {}.'.format(bl)],\n",
    "                 clabels=['Amplitude\\n(Uncalibrated)', 'Amplitude (Jy)', 'Amplitude (Jy)', 'Amplitude (Jy)'],\n",
    "                 clims=[[0, 15], None, [0,15], [0,15]],\n",
    "                 cmaps = ['inferno', 'inferno', 'inferno', 'inferno'],\n",
    "                 extents=[hd_autos.freqs[0] / 1e6, hd_autos.freqs[-1] / 1e6, \n",
    "                          hd_autos.times[-1] - 2458098, hd_autos.times[0] - 2458098],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check our inferred value for the noise on visibilities by checking them against a sequential difference of the data. In this case, we use `hera_cal.noise.interleaved_noise_variance_estimate()` to estimate the noise on the data by subtracting 0.5 times the next and previous channels from the data. Averaging in time over the file, we see that these two estimates of the noise agree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate noise from visibility data using interleaved frequencies\n",
    "from hera_cal.noise import interleaved_noise_variance_estimate\n",
    "data_with_nans = deepcopy(sac_data[bl])\n",
    "data_with_nans[sac_data_flags[bl]] = np.nan\n",
    "noise_var_est = interleaved_noise_variance_estimate(data_with_nans, kernel=[[-.5, 1, -.5]])\n",
    "with np.errstate(divide='ignore', invalid='ignore'):\n",
    "    interleaved_noise = np.sqrt(np.nanmean(noise_var_est, axis=0))\n",
    "\n",
    "# Estimate noise on baseline using autocorrelations\n",
    "var_with_nans = noise[auto_bl1] * noise[auto_bl2]\n",
    "var_with_nans[sac_data_flags[bl]] = np.nan\n",
    "autocorrelation_noise = np.sqrt(np.nanmean(var_with_nans, axis=0))\n",
    "\n",
    "# Plot Results\n",
    "plt.figure(figsize=(12,3), dpi=100)\n",
    "plt.plot(hd.freqs / 1e6, interleaved_noise, label='Interleaved Noise Estimate')\n",
    "plt.plot(hd.freqs / 1e6, autocorrelation_noise, lw=3, label='Noise Inferred from Autocorrelations')\n",
    "plt.xlim(100,200)\n",
    "plt.xlabel('Frequency (MHz)')\n",
    "plt.ylabel('Amplitude (Jy)')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection Fitting\n",
    "\n",
    "**New in IDR 2.2:**\n",
    "\n",
    "Calibration smoothing erases small scale spectral structure in calibration solutions, some of which we believe is real. For example, we see clear evidence from the autocorrelations of ripples in the calibration solutions attributable to reflection systematics. An extra trip down and back up 150 m cables produce delays of 1250 ns, assuming a speed of light in the cables of $0.8c$. Such reflections were seen in every antenna at different delays and amplitudes in [HERA Memo #64](http://reionization.org/wp-content/uploads/2013/03/HERA064_reflection_memo.pdf).\n",
    "\n",
    "In the two forthcoming Kern et al. papers on HERA systematics, we develop and test a method for mitigating these systematics by fitting the reflection in delay space and then running an iterative, non-linear optimizer to refine the estimate of the reflection delay, amplitude, and phase. \n",
    "\n",
    "In theory, the calibration solutions found for the cable reflections could be incorporated into the final, smoothed calibration solution by multiplying (an example of which we show below). In practice, we feel that this move is risky until the stability of the fits and of the cable reflections themselves are more thoroughly explored. Therefore, we provide calibration solutions of just cable reflections in `.reflections.calfits` files in order to support open research questions about these reflections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reflections_file = os.path.join(analysis_folder, 'zen.{}.HH.reflections.calfits'.format(JD))\n",
    "hc = HERACal(reflections_file)\n",
    "rfl_gains, rfl_flags, _, _ = hc.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waterfall_column([np.abs(rfl_gains[ant]), np.abs(rfl_gains[ant] * sac_gains[ant])],\n",
    "                 [rfl_flags[ant], sac_flags[ant]],\n",
    "                 ['Cable Reflection Fit: {}'.format(ant), 'Smoothcal x Reflections: {}'.format(ant)],\n",
    "                 clabels=['Gain Amplitude\\n(Unitless)', 'Gain Amplitude\\n(Unitless)'],\n",
    "                 cmaps = ['inferno', 'inferno'],\n",
    "                 extents=[hd.freqs[0] / 1e6, hd.freqs[-1] / 1e6, hd.times[-1] - 2458098, hd.times[0] - 2458098],\n",
    "                 figsize=(12,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Frequency Synthesis Imaging with CASA\n",
    "\n",
    "**New in IDR 2.2:**\n",
    "\n",
    "For this IDR, all files are also passed through a multi-frequency synthesis imagine routine in CASA. These images are largely for diagnostic purposes, they are not meant for direct comparison to external catalogs or for any sort of power spectrum analysis. We image both in instrumental polarizations, i.e. `ee` and `nn`, and in pseudo-Stokes I, Q, U, and V. The various imaging products are stored as `.fits` in the `*.calibrated.uvh5_image/` folder corresponding to each file. \n",
    "\n",
    "The image products are dirty images (no CLEANing has been done, i.e. `niter=0`) and are not primary-beam corrected. We use [Briggs weighting](https://casa.nrao.edu/Release3.3.0/docs/UserMan/UserMansu247.html) with `robust=0`. We synthesize the image using channels 150 to 900, corresponding to 114.6 - 187.9 MHz. The images are 512 by 512 pixels centered at zenith with 500 arcsecond pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits\n",
    "from astropy import wcs\n",
    "from hera_cal.utils import polnum2str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_fits = os.path.join(analysis_folder, 'zen.{0}.HH.calibrated.uvh5_image/\\\n",
    "zen.{0}.HH.calibrated.uvh5.image.image.fits'.format(JD))\n",
    "hdulist = fits.open(image_fits)\n",
    "coords = wcs.WCS(hdulist[0].header, naxis=[wcs.WCSSUB_CELESTIAL])\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10), dpi=100, subplot_kw={'projection': coords})\n",
    "plt.subplots_adjust(hspace=0, wspace=.3)\n",
    "for i, (ax, image_data) in enumerate(zip(np.ravel(axes), hdulist[0].data[:, 0, ...])):\n",
    "    im = ax.imshow(image_data, cmap='inferno')\n",
    "    ax.grid()\n",
    "    ax.set_xlim([150, image_data.shape[0] - 150])\n",
    "    ax.set_ylim([150, image_data.shape[1] - 150])\n",
    "    pol = polnum2str(int(i * hdulist[0].header['CDELT4'] + hdulist[0].header['CRVAL4'])).replace('p', 'Pseudo-')\n",
    "    plt.colorbar(im, ax=ax, label='Jy/Beam', fraction=0.046, pad=0.04)\n",
    "    props = dict(boxstyle='round', facecolor='white', alpha=0.8)\n",
    "    ax.text(0.05, 0.95, 'Stokes ' + pol, transform=ax.transAxes, fontsize=14, verticalalignment='top', bbox=props)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This field can be compared to the one used in [HERA Memo #42](http://reionization.org/wp-content/uploads/2013/03/abscal_memo.pdf) to calibrate IDR 2.1, thought a different field was used to calibrate this data (see above). Overall, the agreement in terms of source positions and flux scale is pretty good, though a more detailed analysis of the image data products is warranted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LST-Binned Data Products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LST-Binning\n",
    "\n",
    "Both before and after delay filtering, the data is LST-binned into a single group and into two groups of interleaved nights. During LST binning, each integration is assigned to the the nearest LST bin, but is rephased to account for the slight difference in LST between its bin center and the LST bin's center. LST bins were chosen to be twice as long as raw data bins (21.475 seconds = 0.00156597 radians of a sidreal day), so every LST bin gets two data points for every input night.\n",
    "\n",
    "Next, the list of rephased data points that would go into any given LST and frequency bin are examined for outliers. For relatively robust outlier detection, we compute the [median absoute deviation (MAD)](https://en.wikipedia.org/wiki/Median_absolute_deviation) of all the points that are to be binned together and use that to compute a modified z-score. As long as there are at least 5 data points to bin together, we then flag all data that with a modified z-score greater than 5, which corresponds to $5\\sigma$ outliers if the distribution were Gaussian.\n",
    "\n",
    "Finally, we compute the standard deviation of both the real and imaginary parts of the unflagged data and store those in the `.STD.` file. We also save the number of samples that went into each binned data point in both the `.LST.` and `.STD.` files. We do not further perform a cut on the number of samples or any other form of post-LST binning RFI flagging. The procedure is repeated with the delay-filtered data as well.\n",
    "\n",
    "For the nights split into two groups, we have picked:\n",
    "* grp1: 2458098, 2458101, 2458103, 2458105, 2458107, 2458109, 2458111, 2458113, 2458115\n",
    "* grp2: 2458099, 2458102, 2458104, 2458106, 2458108, 2458110, 2458112, 2458114, 2458116\n",
    "\n",
    "The data can be found in `/lustre/aoc/projects/hera/H1C_IDR2/IDR2_1/LSTBIN/{one/two}_group/grp{N}(_dly)/` and have the format like `zen.grp1.of1.LST.1.31552.HH.OCRSL.uvh5`. This means:\n",
    "\n",
    "* `grp1`: 1-indexed group of nights that the LST binned data products are divided into\n",
    "* `of2`: number of groups of nights that the data was divided into before separately LST-binning each group together\n",
    "* `LST`: LST-binned data. The other alternative is `STD`, which encodes the standard deviation of the real and imaginary parts (separately) of the binned data in its real and imaginary data array\n",
    "* `1.31552`: starting LST of the data in radians\n",
    "* `OCRSL`: this denotes calibrated, RFI-flagged, LST-binned data, but not delay-filtered. The LST-binned results also include `OCRSDL` files, which have been delay-filtered before LST-binning. Those are in folders that look like `grp1_dly` instead of just `grp1`.  **L** is for **L**ST-binning.\n",
    "\n",
    "Here we show a comparison of LST-binned data with data from a single night:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstbin_dir = '/lustre/aoc/projects/hera/H1C_IDR2/IDR2_2/LSTBIN/one_group/grp1' # at NRAO\n",
    "if local_comp:\n",
    "    lstbin_dir = analysis_folder # local copy\n",
    "lstbinned_file = os.path.join(lstbin_dir, 'zen.grp1.of1.LST.1.40949.HH.OCRSL.uvh5')\n",
    "hd_lst = HERAData(lstbinned_file)\n",
    "lst_data, lst_flags, lst_nsamples = hd_lst.read(bls=[bl, auto_bl1, auto_bl2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare redundantly and then absolutely calibrated data to externally calibrated reference\n",
    "waterfall_column([np.angle(sac_data[bl]), np.angle(lst_data[bl]), lst_nsamples[bl]],\n",
    "                 [sac_data_flags[bl], lst_flags[bl], np.zeros_like(lst_flags[bl])],\n",
    "                 ['2458098: {}'.format(bl), 'LST-binned: {}'.format(bl), 'LST-binning samples: {}'.format(bl)],\n",
    "                 clabels=['Phase (Radians)', 'Phase (Radians)', 'Number of Samples'],\n",
    "                 ylabel='LST (Radians)',\n",
    "                 cmaps = ['twilight', 'twilight', 'inferno'],\n",
    "                 ylims = [[hd_lst.lsts[-1], hd_lst.lsts[0]], None, None],                 \n",
    "                 extents=[[hd.freqs[0] / 1e6, hd.freqs[-1] / 1e6, hd.lsts[-1], hd.lsts[0]], \n",
    "                          [hd_lst.freqs[0] / 1e6, hd_lst.freqs[-1] / 1e6, hd_lst.lsts[-1], hd_lst.lsts[0]],\n",
    "                          [hd_lst.freqs[0] / 1e6, hd_lst.freqs[-1] / 1e6, hd_lst.lsts[-1], hd_lst.lsts[0]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the these results look reasonable. The flagging is substantially more aggressive than in IDR2.1, which is reflected in the lower and more variable number of samples above, through there are still a couple of \"clean\" windows around 125 MHz and around 160 MHz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CPS of two groups of nights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not local_comp:\n",
    "    lst_binned_file1 = os.path.join(os.path.dirname(os.path.dirname(lstbin_dir)), 'two_group/grp1/zen.grp1.of2.LST.1.31552.HH.OCRSL.uvh5')\n",
    "    lst_binned_file2 = os.path.join(os.path.dirname(os.path.dirname(lstbin_dir)), 'two_group/grp2/zen.grp2.of2.LST.1.31552.HH.OCRSL.uvh5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cpspec_calc(dfile1, dfile2=None):\n",
    "    \n",
    "#     if not dfile2:\n",
    "#         # Load data into UVData objects\n",
    "#         uvd = UVData()\n",
    "#         uvd.read_uvh5(dfile)\n",
    "\n",
    "#         # find conversion factor from Jy to mK\n",
    "#         Jy_to_mK = uvb.Jy_to_mK(np.unique(uvd.freq_array), pol='xx')\n",
    "\n",
    "#         # reshape to appropriately match a UVData.data_array object and multiply in!\n",
    "#         uvd.data_array *= Jy_to_mK[None, None, :, None]\n",
    "\n",
    "#         # We only have 1 data file here, so slide the time axis by one integration \n",
    "#         # to avoid noise bias (not normally needed!)\n",
    "#         uvd1 = uvd.select(times=np.unique(uvd.time_array)[:-1:2], inplace=False)\n",
    "#         uvd2 = uvd.select(times=np.unique(uvd.time_array)[1::2], inplace=False)\n",
    "        \n",
    "#     else:\n",
    "#         uv_dict = {}\n",
    "#         for i, dfile in enumerate((dfile1, dfile2)):\n",
    "#             uvd = UVData()\n",
    "#             uvd.read_uvh5(dfile)\n",
    "            \n",
    "#             # find conversion factor from Jy to mK\n",
    "#             Jy_to_mK = uvb.Jy_to_mK(np.unique(uvd.freq_array), pol='xx')\n",
    "\n",
    "#             # reshape to appropriately match a UVData.data_array object and multiply in!\n",
    "#             uvd.data_array *= Jy_to_mK[None, None, :, None]\n",
    "            \n",
    "#             uv_dict['uvd{}'.format(i+1)] = uvd\n",
    "            \n",
    "#         uvd1 = uv_dict['uvd1']\n",
    "#         uvd2 = uv_dict['uvd2'] \n",
    "\n",
    "#     # Create a new PSpecData object\n",
    "#     ds = hp.PSpecData(dsets=[uvd1, uvd2], wgts=[None, None], beam=uvb)\n",
    "#     # Here had to go into pyuvdata utils and change pol dicts s.t. xx->ee, yy->nn, xy->en, yx->ne etc.\n",
    "#     # Will have to reinstall pyuvdata once this is fixed\n",
    "    \n",
    "    \n",
    "#     # Because we are forming power spectra between datasets that are offset in LST there will be some\n",
    "#     # level of decoherence (and therefore signal loss) of the EoR signal. For short baselines and small\n",
    "#     # LST offsets this is typically negligible, but it is still good to try to recover what coherency\n",
    "#     # we can, simply by phasing (i.e. fringe-stopping) the datasets before forming the power spectra. \n",
    "#     # This can be done with the rephase_to_dset method, and can only be done once.    \n",
    "#     ds.rephase_to_dset(0) # Phase to the zeroth dataset\n",
    "    \n",
    "#     # change units of UVData objects\n",
    "#     ds.dsets[0].vis_units = 'mK'\n",
    "#     ds.dsets[1].vis_units = 'mK'\n",
    "\n",
    "#     # Find list of baselines pairs to calculate power spectra for\n",
    "#     uvd_ant_copy = uvd1.copy()\n",
    "#     # uvd_ant_copy.unphase_to_drift(use_ant_pos=True)\n",
    "#     tol = 1.0  # Tolerance in meters\n",
    "#     uvd_ant_copy.select(times=uvd_ant_copy.time_array[0])\n",
    "\n",
    "    \n",
    "#     # really should find the baseline groups for the lst_binned_files separately... \n",
    "#     # asumme they share the same baselines\n",
    "    \n",
    "#     # Returned values: list of redundant groups, corresponding mean baseline vectors, baseline lengths. \n",
    "#     # No conjugates included, so conjugates is None.\n",
    "#     baseline_groups, vec_bin_centers, lengths = uvutils.get_baseline_redundancies(uvd_ant_copy.baseline_array, \\\n",
    "#                                                                                   uvd_ant_copy.uvw_array, tol=tol)\n",
    "\n",
    "#     # Selecting shortest (~14.6m) EW baselines group\n",
    "#     if len(baseline_groups) == len([bl for bl_group in baseline_groups for bl in bl_group]):\n",
    "#         # Check to see if baselines haven't already been aggregated by group - this is done in omnical\n",
    "#         # where only 'only one baseline per unique separation' is kept\n",
    "#         bls_to_include = baseline_groups[0]\n",
    "#         bls1 = [uvutils.baseline_to_antnums(bls_to_include[0], len(uvd1.get_ants()))]\n",
    "#         bls2 = bls1\n",
    "        \n",
    "#     else:\n",
    "#         bls_to_include = baseline_groups[1]\n",
    "        \n",
    "#         # Converting to antnum tuples to be used to construct_blpairs later on\n",
    "#         # How do errors reduce when all cross-correlations are averaged?\n",
    "#         ant_pairs_to_include = [uvutils.baseline_to_antnums(i, len(uvd1.get_ants())) for i in bls_to_include]\n",
    "#         bls1, bls2, blp = hp.utils.construct_blpairs(ant_pairs_to_include, exclude_permutations=False, exclude_auto_bls=True)\n",
    "\n",
    "#     # Power spectrum calculation\n",
    "#     uvp = ds.pspec(bls1, bls2, (0, 1), [('xx', 'xx')], spw_ranges=[(300, 400), (600,721)], input_data_weight='identity', norm='I', \n",
    "#                    taper='blackman-harris', verbose=False)\n",
    "\n",
    "#     blpairs = np.unique(uvp.blpair_array)\n",
    "#     blps = list(blpairs)\n",
    "    \n",
    "#     return uvp, blps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not local_comp:\n",
    "#     uvp, blps = cpspec_calc(lst_binned_file1, lst_binned_file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not local_comp:\n",
    "#     # Plot the spectra averaged over baseline-pairs and times\n",
    "#     ax = hp.plot.delay_spectrum(uvp, [blps,], spw=0, pol=('xx','xx'), average_blpairs=True, average_times=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compare the observed noise level to what we excepted from the autocorrelations (both from individual days and from the LST-binned autocorrelations). This match is pretty good, though there's still some evidence for temporal structure in the STD files, especially in the real/imaginary components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lststd_file = os.path.join(lstbin_dir, 'zen.grp1.of1.STD.1.40949.HH.OCRSL.uvh5')\n",
    "hd_std = HERAData(lststd_file)\n",
    "std_data, std_flags, _ = hd_std.read(bls=[bl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict noise from lst-binned autocorrelations\n",
    "from hera_cal.noise import predict_noise_variance_from_autos\n",
    "tint = np.median(np.diff(hd.times)) * 24 * 3600\n",
    "lst_bl_noise = np.sqrt(predict_noise_variance_from_autos(bl, lst_data, dt=tint))\n",
    "lst_bl_flags = lst_flags[bl] | lst_flags[auto_bl1] | lst_flags[auto_bl2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare redundantly and then absolutely calibrated data to externally calibrated reference\n",
    "waterfall_column([np.abs(bl_noise), np.abs(lst_bl_noise), np.abs(std_data[bl])],\n",
    "                 [bl_noise_flags, lst_bl_flags, std_flags[bl]],\n",
    "                 ['Predicted Noise from Autos: ' + str(bl), \n",
    "                  'Predicted Noise from LST-Binned Autos: ' + str(bl),\n",
    "                  '|STDEV| over Nights: {}'.format(bl)],\n",
    "                 clabels=['$\\sigma$ (Jy)', '$\\sigma$ (Jy)', '$\\sigma$ (Jy)'],\n",
    "                 ylabel='LST (Radians)',\n",
    "                 clims=[[0,15], [0,15], [0,15]],\n",
    "                 ylims = [[hd_lst.lsts[-1], hd_lst.lsts[0]], None, None],\n",
    "                 cmaps = ['inferno', 'inferno', 'inferno'],\n",
    "                 extents=[[hd.freqs[0] / 1e6, hd.freqs[-1] / 1e6, hd.lsts[-1], hd.lsts[0]],\n",
    "                          [hd_std.freqs[0] / 1e6, hd_std.freqs[-1] / 1e6, hd_std.lsts[-1], hd_std.lsts[0]],\n",
    "                          [hd_std.freqs[0] / 1e6, hd_std.freqs[-1] / 1e6, hd_std.lsts[-1], hd_std.lsts[0]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Path Forward\n",
    "\n",
    "The goal of IDR 2.2 is to provide calibration, flagging, and LST-binning to support an astrophysically interesting power spectrum upper limit. It is not meant to be the last word in HERA analysis. Hopefully, this release is \"good enough\" to move forward, but it is important to document the issues as they arise and continue to cross-check our results and better understand the systematics in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Known Issues and Future Improvements\n",
    "\n",
    "* **Abscal model flagging**: The abscal model shows evidence for low-level RFI going unflagged, contaminating nearly channels due to the low-pass delay filter. While our subsequent flagging algorithm appears to catch these outliers, better flagging of the externally calibrated data set would remove the the need for this. Also, we expect this issue to lead to RFI flags that repeat at the same LST day after day. We have an [open Github issue](https://github.com/HERA-Team/hera_cal/issues/437) to address this. \n",
    "\n",
    "\n",
    "* **Antennas flagged in abscal model are flagged throughout the data**: As it stands, any antennas that are completely flagged in the abscal model are flagged in the data as well. This is probably too conservative. We have an [open Github issue](https://github.com/HERA-Team/hera_cal/issues/466) to address this. \n",
    "\n",
    "\n",
    "* **Abscal model improvements:** A number possible improvements to the externally calibrated abscal model have been suggested. These include:\n",
    "    * Fringe-rate filtering the abscal model (along with low-pass delay filtering it) to improve signal to noise.\n",
    "    * Including `en` and `ne` visibilities. As of now, our the externally calibrated data used as our abscal model has no polarization information, so we cannot calibrate the absolute phase of x relative to y. This makes pseudo-Stokes U and V difficult to interpret. \n",
    "    * Calibrating off a limited set of trustworthy, redundantly-averaged (or possibly simulated) baselines. This might be the only practicable way to make abscal work for RTP in the next season. \n",
    "    * Abscal $\\chi^2$ is not normalized nor particularly well-understood. We could use a systematic investigation of the discrepancies between the abscal model and the absolutely calibrated data.\n",
    "\n",
    "\n",
    "* **Cross-talk**. Even after the delay filter, we see clear evidence of persistent structure on short baselines. This was explored in the two forthcoming Kern et al. papers. At this point, at this point, our plan is to apply these techniques to LST-binned data before forming power spectra, though it remains an open question whether this is best done before LST-binning---which is to say, does the cross-talk repeat from day to day?\n",
    "\n",
    "\n",
    "* **Fringe structure in STD files.** The real and imaginary components of the STD files appear to have fringe structure in them. This is likely attributable to un/mis-calibrated gain variations from day to day, though we don't know that for sure. Does this mean that data is not integrating down properly? Is it also a problem in the delay-filtered, LST-binned, data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open Questions\n",
    "\n",
    "\n",
    "* **Cable Reflections**. We solve for cable reflections at every integration, but our current plan is to ignore these results and only solve for and then calibrate out cable reflections in LST-binned data. How accurate and reliable are our cable reflection fits in this IDR? And do the cable reflections exhibit any temporal evolution or additional spectral structure (beyond the three parameter model used to constrain them?)\n",
    "\n",
    "* **Temporal structure**. How stable are the calibration solutions in time (per feed)? What kind of temporal structure do they exhibit and how repeatable is that structure from day to day? What is the optimal temporal smoothing scale for calibration solutions? (This is under investigation by UCB undergraduate Max Lee and a memo on this question is forthcoming.)\n",
    "\n",
    "* **Spectral structure**. What kind of spectral structure do our calibration solutions exhibit? How does it vary from antenna to antenna? From integration to integration? From day to day? Is the spectral structure in our calibration solutions consistent with that expected from beam modeling of spectral structure? What is the optimal spectral smoothing scale for calibration solutions? Is there some spectral form other than delay modes on which we should be filtering (e.g. Hermite polynomials? PCA/Weiner filter?).\n",
    "\n",
    "* **RFI Report**. What are the persistent sources of RFI? How much of the band are we losing? Are there any major transient broadband events and can they be tracked to any source (e.g. MeerKAT, [as we saw in 2458109](https://github.com/HERA-Team/HERA_Commissioning/issues/114))?\n",
    "\n",
    "* **Post-LST RFI**. Is there any low level RFI that we are not detecting? Are we overflagging? Does the LST-binned data product show any evidence for RFI? Should we be removing RFI after LST binning?\n",
    "\n",
    "* **Versus Sky-Based Calibration**. How do the calibration solutions compare to those generated purely from a sky-based calibration? \n",
    "\n",
    "* **Relative e-to-n Phase Calibration**. Right now, our abscal model has no en or ne visibilities, which means that the relative phase of x to y as a function of frequency is uncalibrated. Is this a problem? How is it affecting Stokes U and V?\n",
    "\n",
    "* **Imaging**. We have produced MFS images, but we have not examined them systematically. Do our maps look reasonable? Do source fluxes and spectra look right? What about psuedo-Stokes I, Q, U, and V images?\n",
    "\n",
    "* **Noise integration.** Does the noise integrate down from day to day? \n",
    "\n",
    "* **LST results.** Does the difference between a given day's data and the corresponding LST-binned data look noise-like? Are there some days that are worse than others?\n",
    "\n",
    "* **Redundancy.** How redundant are redundant baselines? How do we interpret $\\chi^2$? Does the noise integrate down inside redundant baseline groups?\n",
    "\n",
    "* **Repeatability of calibrated data**. How repeatable are redundant baselines from day to day after calibration? Can or should we be LST-binning updated omnical visibility solutions?\n",
    "\n",
    "* **Beams**. Does our beam model make sense? Can we constrain it with foregrounds moving through the beam?\n",
    "\n",
    "* **Delay filtering**. We use a CLEAN-based filtering in calibration smoothing and in the delay transform steps. Is that the best algorithm for it? Are we using the best tapering function?\n",
    "\n",
    "* **Delay spectrum.** How does the delay spectrum look? Is it noise-like at k~.2 and beyond after foreground filtering on the EoR baselines? How does it depend on LST?\n",
    "\n",
    "\n",
    "If you want to work on any of these questions (or already are!) or are curious about this IDR, please join our  HERA analysis telecon on [Tuesdays at 10am Pacific on Zoom](https://berkeley.zoom.us/j/446119451) or the [#hera-analysis Slack channel](https://eoranalysis.slack.com/messages/C3ZPGMG3E). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hera",
   "language": "python",
   "name": "hera"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "230.969px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
